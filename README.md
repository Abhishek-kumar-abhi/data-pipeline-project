End-to-End Data Pipeline using FastAPI, PostgreSQL, Prefect & Grafana


Project Overview

This project demonstrates a complete data engineering pipeline using modern open-source tools.
It reads raw data from multiple sources (CSV and JSON API), performs data cleaning and transformation, loads the results into a PostgreSQL database, and visualizes analytics dashboards in Grafana.

The pipeline is automated and orchestrated using Prefect, ensuring hourly updates.

Components Used

FastAPI ‚Üí Serves JSON API for one dataset

Pandas ‚Üí For data cleaning and merging

PostgreSQL ‚Üí For storing and querying processed data

Prefect ‚Üí For orchestration and hourly scheduling

Grafana ‚Üí For creating interactive dashboards


Installation & Setup
1Ô∏è‚É£ Clone Repository & Install Requirements
git clone <repo_url>
cd data_pipeline_project
python -m venv venv
source venv/bin/activate        # (Windows: venv\Scripts\activate)
pip install -r requirements.txt

2Ô∏è‚É£ Setup PostgreSQL

Make sure PostgreSQL is running locally.

Example configuration (.env file):

DB_NAME=olist_db
DB_USER=postgres
DB_PASSWORD=ABHI@7M
DB_HOST=localhost
DB_PORT=5432

3Ô∏è‚É£ Run FastAPI Server

Serve one dataset as a JSON API:

cd app
uvicorn main:app --reload


Access it at üëâ http://127.0.0.1:8000/data

4Ô∏è‚É£ Run Prefect Flow

Run the Prefect server:

prefect server start


Deploy and run the data pipeline:

python prefect_flow.py


To schedule every 1 hour:

prefect deployment run "data_pipeline_flow"
prefect agent start

5Ô∏è‚É£ Connect PostgreSQL to Grafana

Go to https://grafana.com/products/cloud/
 ‚Üí Start for Free

Add a new PostgreSQL data source

Fill credentials:

Host: localhost
Database: analytics_db
User: postgres
Password: postgres
SSL Mode: disable


Click ‚ÄúSave & Test‚Äù


Chart Type	Data Source Table	Example Query	Description
Bar Chart	sales_summary	SELECT region, SUM(total_revenue) FROM sales_summary GROUP BY region;	Revenue by region
Pie Chart	sales_summary	SELECT region, COUNT(*) FROM sales_summary GROUP BY region;	Orders per region
Line Chart	staging_data	SELECT order_date, SUM(payment_value) FROM staging_data GROUP BY order_date ORDER BY order_date;	Daily sales trend
Gauge / Stat Panel	delivery_performance	SELECT AVG(avg_payment) FROM delivery_performance;	Average payment value


Pipeline Workflow

Data Extraction:

Reads CSV data from /data/olist_order_payments_dataset.csv

Reads JSON API data from FastAPI endpoint /data

Data Transformation:

Cleans missing values

Merges sources on common keys (e.g., order_id, customer_id)

Data Loading:

Inserts cleaned data into PostgreSQL tables:

staging_data

sales_summary

delivery_performance

Analytics Visualization:

Grafana connects to PostgreSQL and visualizes dashboards.

Orchestration:

Prefect runs this pipeline automatically every 1 hour.

‚úÖ Connection Troubleshooting
Checkpoint	Description
üîπ PostgreSQL running	Start using sudo service postgresql start
üîπ Database credentials correct	Match .env and Grafana settings
üîπ Port open	Ensure port 5432 is accessible
üîπ FastAPI reachable	Visit http://127.0.0.1:8000/data
üîπ Prefect flow active	Check with prefect server start


  Requirements

Example requirements.txt:

fastapi
uvicorn
pandas
sqlalchemy
psycopg2-binary
prefect
python-dotenv

Summary

This project demonstrates:

Data ingestion from mixed sources

Automated data engineering with Prefect

Relational modeling in PostgreSQL

Real-time visualization using Grafana Cloud